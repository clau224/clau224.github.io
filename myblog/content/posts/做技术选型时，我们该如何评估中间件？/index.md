

+++

date = '2025-08-26T23:37:54+08:00'
draft = false
title = '做技术选型时，我们该如何评估中间件？'
summary = '在加入L团队后，我负责了一个多路召回的技术设计，主要对Milvus做了技术选型和评估，但部分过程与结论不被团队所认可。这也让我开始思考一个问题...'


+++





## 为什么我想聊聊这个？

在加入L团队后，我负责了一个多路召回的技术设计，在其中，我主要对Milvus做了技术选型和评估，但部分过程与结论不被团队所认可。

我认为核心因素有两个，一是我对业务领域（含上下游链路）的理解不到位，二是有一些团队标准/导向是我未预料到的。

入职一个多月以来，我观察到了与之前M团队间的一些差异点，这让我感到新鲜，但也有稍许不适应，简单说说：

- 性能分析精细度：
  - M团队侧重于功能性验证，对于性能分析，其隶属于技术设计的可行性分析部分，采用三步法：评估未来3年数据增长 + 测出最大QPS/性能拐点 + 制定指标红线（约定红线SOP）。
  - L团队有POC阶段，该阶段专门用于中间件的功能性验证和性能验证。在性能验证上较为细致，会严格使用真实数据量+真实数据分布。各类参数都要进行实验组覆盖，确认自变量对整体性能的影响及趋势。

- 成本控制：
  - M团队在设计阶段，会粗估服务器数量；最终服务器数量的确认，会在测试阶段的性能压测期间进行评估调整。对于服务器数量，一般重点考虑突发情况，只估上限不估下限。同时底层有中间件团队负责快速扩容+技术支持。
  - 而L团队则倾向于极致的成本管理，包括机器型号、所在机房等底层因素都需要纳入考虑。对成本把控较严格，较低的资源使用率会被挑战。

- 方案倾向性：
  - M团队虽然是非业务团队（平台工具），但比较业务导向，更关注需求的合理性、业务对架构的影响。做性能方案选择时，在性能差异不大（一般5%内）的情况下，会倾向于易扩展、易维护、易排查的方案。
  - L团队在做性能分析时，会选择性能最极致的方案；且对延迟较为敏感，评判标准会精细到0.1ms级别。此外，L团队与国外团队存在协作交互，方案会充分参考各方意见。

发现有些事儿没系统思考过。从毕业后开始工作，其实一直身处在较熟悉的环境中，团队流程/标准/导向较为相对固化，很多做法遵循群体潜意识就会得到解决。加入L团队后，诸多问题同时暴露出来，加上本人对新环境的适应能力一般，多少有点难绷。

因此有了本篇blog。我想在这里，搞定一件事情——

所谓南橘北枳，从M团队到L团队，一味延续习惯会出现问题。是否可以梳理出一套方法论，无论在哪个团队，都能高标准完成中间件技术选型？





## 不同业务场景，是否需要差异化评估？

首先，我认为，有差异也有共性。核心都是三要素：业务、技术、成本。

一套方法论不可能生搬硬套到所有团队，不同业务场景对于中间件评估的侧重点会有些许不同；同时，一个团队的业务场景，可能同时包含多种业务特性。

### 0. **必需的基本调研**

   - 功能性匹配：中间件是否支持业务核心需求。
   - 基本可靠性：无单点故障风险、数据不丢失的基础保障能力。
   - 兼容性：与现有技术栈（语言、框架）的集成可行性。
   
### 1. **B端业务**
  企业级应用，业务逻辑复杂。
   - 复杂场景适配：是否支持业务逻辑中的特殊需求（如复杂数据结构等）。
   - 可扩展性：B端业务经常发生功能扩展/变更。对于一些可以预见的产品诉求，中间件是否有能力支持。
   - 运维可控性：B端逻辑复杂，是否提供精细化监控/日志（如业务链路追踪），便于问题定位。

### 2. **C端业务**
  面向用户，并发流量较大。
   - 高性能与吞吐量：C端流量远大于B端，中间件的并发处理能力，能否支撑峰值流量。
   - 低延迟：数据读写 / 消息传递的延迟指标，是否影响用户体验（如 Redis 的响应时间、MQ 的投递延迟）。
   - 弹性伸缩：在流量波动时，能否快速扩缩容（如 Redis 集群分片、MQ 的动态队列扩容）。
   - 快速故障恢复：C 端业务一旦出现故障，影响范围广、用户感知强。那么是否具备快速故障恢复机制呢？

### 3. **创业团队**
  资源有限，快速迭代。
   - 资源占用：要考虑CPU、内存 等资源消耗是否较低，租用太多服务器成本会难以接受。
   - 轻量级部署：是否支持快速搭建（如单节点部署、容器化一键启动），降低初期运维成本。
   - 学习与使用成本：API 是否简洁、是否易于开发者快速上手，减少团队学习投入。

### 4. **金融政务**
  强合规，高安全。
   没做过类似方向，但结合公开的技术分享，我简单梳理出以下几点：
   - 高可靠性与灾备：是否支持多副本、异地容灾（像Redis的主从+哨兵、MQ的多集群同步这种），确保服务不中断。
   - 数据安全性：传输/存储是否支持数据加密、访问权限严格控制。
   - 合规审计：能否记录完整链路的操作日志，满足监管审计要求。

仅停留在宏观视角做分析，往往难以覆盖全部细节。为确保中间件评估的全面性与可操作性，我将进一步拆解每个评估维度，细化为可落地的实施要点，形成相对标准化的 checklist。





## 评估中间件时，有哪些核心纬度？

我梳理出八个纬度，纬度较多，但并不是在每个需求中都要同时考虑。

纬度的列举以及纬度内的实施点，都是参考借鉴他人的优秀设计并结合本人过往工作经验总结而来，会有疏漏，欢迎补充！

### 1. **功能性维度**

你引入某中间件，肯定希望通过它解决某些业务/技术难点。但它是否真的能解决问题呢？需要验证。

- 核心能力覆盖：

  - 【难点梳理】列出必须要解决的具体业务难点，确认目标很重要，避免疏漏和跑偏...
  - 【能力拆解】针对每个难点，拆解为中间件的具体能力需求。这里先只提需求，不要收到具体中间件的能力的影响。
  - 【基础功能验证】写一个dmo，它会帮我们暴露很多问题。
    - 确认数据的适配性，中间件能否完美适配业务数据；如果不能，业务数据能否类型转换后接入中间件。
    - 确认功能特性的正确性：
      - 确定测试数据，使用具备区分度的测试数据进行验证。例如测试存储型中间件时，涉及到query功能验证，那么需要准备好重复率较低的数据集以及对应的Query。
      - 生成验证demo，接入中间件client后可以采用单测+断言进行快速验证。初步判定可行后，再封装中间件Client以复用于开发阶段。
      - 执行链路后结果符合预期，且执行期间无异常/报错。
    - 确认结果的完整性，部分中间件协议不识别某些数据类型，可能出现部分数据丢失的情况。
  - 【异常功能验证】模拟异常情况，尤其是上游链路传递的可能的异常情况。
    - 非法数据处理：传null、类型不匹配会怎样？这决定了校验器的实现。
    - 连接中断恢复：少部分场景需要模拟网络断开的故障，以判定中间件是否符合功能需要，比如DB大事务场景。这也决定了重试、事务等机制的实现。
  - 【边界情况验证】测试功能临界值
    
    不清楚中间件哪些资源存在边界情况，可以咨询LLM。
    
    - 边界数据处理：信息size过大等非常规场景，边界上限是多少，需要如何处理？
    - 资源耗尽测试：部分场景，业务数据正常使用中间件时，某些资源存在瓶颈，比如连接资源超限、线程资源耗尽等情况。这可能影响功能的正确性。

- 深度功能覆盖：在确认能实现业务功能后，它是否真的能上线，还取决于多种因素：

  - 【灰度策略验证】开发到上线之间的多实验版本控制、蓝绿发布等环节，都可能出现中间件无法胜任的情况，需要重点考虑。

    - 如果是存储类型中间件，如何保存多个版本的数据？放一张表，还是设计实验版本表？
      - 放一张表，该表是否过宽？是否支持动态增加字段？表内字段是否有上限？
      - 设计实验版本表，如何平滑切换？如何做数据同步（增量+存量）？
    - 如果是通信类型中间件，如果保证链路隔离？
      - 比如，如何保证链路标识会一直存在？Java里的ThreadLocal等机制是否会导致标识丢失。
      - 如果链路存在nginx等反向代理，如何保证流量分发给预期的下游服务器？
    - 如果是任务调度类型中间件，如何保证各实验链路执行的准确性？
      - 还涉及到业务实现，各实验链路都需要接受调度吗？所以采用广播模式还是单点模式？

  - 【配置灵活性确认】部分场景需要动态配置与能够自定义策略

    - 确认参数能否动态调整：例如动态线程池、动态缓存过期策略、动态MQ消费线程数。

      如果可以动态调整中间件核心参数，那么在故障发生时，就多了一种快速消除故障的手段，比如线程池打满、MQ消费能力不足的场景。

  - 【功能扩展性确认】部分业务场景极其复杂，且随着业务演进只会更复杂，中间件能否撑得起来？

    - 是否支持API 扩展与二次开发支持？比如任务调度中间件开放任务触发规则 API，可实现根据业务数据动态生成调度策略；缓存中间件支持自定义缓存淘汰算法接口。
    - 中间件是否支持通过插件扩展核心功能？这是API网关类型中间件非常值得关注的点。

### 2. **性能维度**

首先列举一下高频出现的性能指标：

| 关注指标     | 指标解释                                                     |
| ------------ | ------------------------------------------------------------ |
| 吞吐量指标   | 单位时间内处理的请求数（QPS） 或 数据量（MB/S）              |
| 响应效率指标 | 延迟（AVG、P99、P90等），长尾延迟发生率（如超过1s的请求占比） |
| 系统资源指标 | 在达到目标性能时，CPU、内存、磁盘IO、网络等硬件资源的占用率  |
| 稳定性指标   | 请求成功率、失败率、超时率，服务可用性                       |
| 资源池指标   | 线程/协程活跃数，连接池线程活跃数，队列堆积数                |

其实，指标大盘很容易搭建（毕竟也可以服务之间互相参考），真正要在性能测试中投入人力设计的是对照实验。

在对照实验中，又以自变量、因变量、控制变量最为重要。那么，我们应该考虑哪些因素作为我们实验变量呢？

- 自变量的考虑要素

  - 硬件资源：核心关注 CPU（需明确型号及核心 / 线程数）、内存（容量及频率）；部分中间件（如消息队列、缓存）需额外考量磁盘 I/O 性能（读写速率、IOPS）及存储介质（HDD/SSD）。
  - 中间件版本：不同版本的中间件在性能优化、功能支持上存在差异。
  - 请求并发度：以虚拟用户数（VUS）为核心指标，需匹配实际业务场景的并发量级。
  - 处理并发度：重点关注连接池（如数据库连接池、HTTP 连接池）、线程池的核心参数（核心线程数、最大线程数、队列容量）；同时需纳入批处理 size（如数据批量读写大小）的影响。
  - 压力模型：阶梯加压（最常用，测试性能拐点）/ 峰值持续 / 脉冲加压。
  - 存储类中间件：
    - 字段类型：直接影响检索效率（如数值型 vs 字符串型查询耗时差异）。
    - 索引结构：需覆盖单字段索引、联合索引（含索引顺序），明确索引对查询的加速 / 冗余影响。
    - 表结构：包含分表策略（垂直分表 / 水平分表）、存储引擎选型（如 InnoDB/MyISAM）、页大小等关键参数。
    - 数据量：需划分不同数据量级（如 10 万级、100 万级），测试数据量增长对性能的衰减趋势。
    - 查询方式：区分等值查询、范围查询、模糊查询等，明确不同方式的性能差异。
    - 查询条件：影响索引命中与否及数据扫描范围。
      - 条件复杂度：设置单条件查询（如仅基于主键查询）、多条件并列查询（AND 连接）等不同复杂度场景。
      - 返回字段影响：设计完全覆盖索引查询、部分覆盖索引查询，验证返回字段对查询效率的影响程度；设计不同数量返回字段数，确定数据库IO对性能的影响。
      - 数据过滤范围：通过调整查询条件的数据过滤比例，如查询 1%、10%、50% 的数据量，观察数据范围扩大时查询性能的衰减趋势，定位性能瓶颈。
      - 函数使用场景：加入在查询条件中使用聚合函数（如 SUM、COUNT）、字符串函数（如 SUBSTRING）等情况，评估函数计算开销对查询性能的影响，确定是否导致索引无法使用。
  - 通信类中间件：
    - todo

- 因变量的考虑要素

  因变量，就是上文提到的吞吐量、响应效率、系统资源、稳定性、资源池等指标，但需要和自变量建立映射关系，例如可以通过折线图等方式，直观展示因变量与自变量间的某种关系。

  - 可视化方案：

    - 折线图：

    - todo：各场景最佳展示方案。

  - 趋势解读
    关于趋势，一般有以下几种：
    - 单调性：因变量会随自变量增加，而持续上升、下降或者波动。例如，并发数增加时，延迟持续上升，因为存在资源竞争。
    - 饱和点：因变量不再随自变量变化的临界点甚至是拐点。例如，并发数 > 1500 时，吞吐量稳定在 2500TPS，说明服务端处理能力达上限。再次提升并发数，吞吐量可能会下降，说明服务端趋于崩溃。
    - 异常波动：趋势中突然出现的非预期变化。例如，延迟在VUS = 300 时突然跳跃，说明可能触发了数据库连接池耗尽。

- 控制变量的考虑要素
  - 非自变量外的其他变量，都需要控制保持一致。



### 3. **可靠性维度**

- 故障容错：中间件出现故障不可避免，是否能自动恢复或降级。
  - 单点故障处理：
    - 是否有多节点冗余，是否支持自动选举新节点接管故障节点？
    - 故障检测机制：如何判断该中间件服务down掉了，或者处于一个错误的状态？
  - 服务器故障处理：
    - 服务器宕机：进程重启后，是否能通过日志/checkpoint 快速恢复状态？数据是否会造成丢失？
    - CPU/内存资源过载时，是否有保护机制（如拒绝策略、降级非核心功能等）？
  - 故障灾备能力：
    - 是否支持数据备份？备份频率？
    - 是否有灾备演练与预案？
- 可用性：能反应长期运行的稳定程度。
  - 该中间件SLA是多少？平均无故障时间和平均恢复时间是多少？
  - 是否有成功应用于大规模生产环境的先前案例？
  - 长期运行可能存在版本升级，是否支持平滑升级？
- 数据一致性：在分布式场景或异常中断时，数据是否会丢失、错乱（如是否支持事务、数据同步策略）。
  - 支持怎样的级别，强一致性还是最终一致性？
  - 是否支持事务？事务如果超时是怎么处理的？
  - 出现了网络中断，恢复后该中间件会怎么处理？

### 4. **成本维度**

- 直接成本：部署所需的硬件 / 云资源费用、商业授权费用。
  - 硬件/云服务器成本
    - 服务器配置+服务器数量：在估算配置时，要考虑单节点需要的CPU、内存还有网络带宽。
    - 资源利用率：考虑CPU/内存等利用率是否在合理区间？过低会产生资源浪费，过高可能会导致服务出现不可用隐患。目前所在的L团队，CPU使用率控制在不超过60%。在此条件下尽可能保持较少的Pod数量。
    - 存储成本：数据持久化是否支持分层存储（冷热数据分离）；备份数据存储周期，以及单位容量的价格。

  - 软件订阅付费成本
    - 确认软件的收费模式：如果是商业版本，可能会按照节点数/核心数/吞吐量计费；还有可能采用SaaS订阅模式，会按照调用量或者租户来收费。
    - 附加服务成本：技术支持、升级维护是否收费？这就不得不提Jfrog了，在X团队时被坑的很惨，以色列人还是会赚钱的。

  - 网络与流量成本
    - 跨机房/跨地域数据交互的流量费用。这个也是我在加入L团队后，才发现AWS跨机房是收费的。
    - 部分中间件（如API网关）可能采用流量计费或请求计费模式。

- 间接成本：学习成本（团队培训）、运维成本（人力投入）、迁移成本（替换旧系统的代价）。
  - 不要小看这个成本，在资源有限的团队，人力成本是开支大头。在间接成本较高的情况下，如果踩坑了，会让资源有限的团队进入万劫不复的境地。


### 5. **扩展性维度**

- 规模扩展：随着业务发展，数据量/并发度会越来越高，能通过增加节点数量提升可用性&性能，很关键。
  - 集群部署能力：是否多节点；要避免伪集群。
  - 分片机制：分片策略是怎样的；是否支持自动重分片；分片容量/分片数是否有上限。
  - 集群一致性保障：各节点间的数据同步机制；多节点场景下的脑裂问题。
  - 数据量扩展：通常和上面的分片挂钩，需要考虑该中间件的单机/集群容量上限。以及是否支持数据归档、清理。
  - 扩展自动化：是否支持自动扩缩容？扩缩绒触发条件是否支持配置等等。

- 功能扩展：
  - 插件化能力：是否支持不重启中间件即可启用/禁用插件功能？是否可以自定义插件？
  - 接口扩展：是否开放核心能力 API？新版本是否兼容旧版本接口？
  - 业务定制化支持：是否支持自定义数据Schema？是否支持自定义规则/路由？


### 6. **兼容性维度**

- 技术栈适配：

  - 是否支持你团队的语言、框架、数据库等现有技术栈，可以先写一个demo跑通。
  - API 兼容性，协议是否支持。一般都提供client，或者提供http/rpc方式调用，确认即可。

- 环境适配：

  - 是否兼容不同操作系统，例如Linux系统版本等；以及芯片的兼容性，x86、ARM架构等。

- 版本兼容：

  在升级中间件版本时，要重点考虑此要素！不同版本间的是否能平滑升级。

  - API 兼容性：新版本是否保留旧版本 API 接口；或者是否提供兼容层；废弃接口是否有过渡期等等。
  - 数据格式：存储格式升级是否兼容旧版本；数据怎么迁移，有迁移工具？
  - 配置兼容：配置项是否有新增、删除或重命名，旧版本配置项是否暂时先兼容？不兼容，怎么迁移？
  - 依赖库：升级后依赖库版本是否变化，是否会对现有项目的依赖树产生影响？有幸升版过一次，这类问题非常隐蔽。

### 7. **安全性维度**

一般考虑比较少，且会有专门的安全团队负责，先跳过。

### 8. **生态与易用性维度**

它重要又不太重要，毕竟我们做技术选型时，一般不会挑小众中间件。常见的技术架构中，使用的中间件都是高频出现的，我们一般不会是第一个趟坑的人。关于生态和易用性，这里我咨询了下LLM，整理出我认为比较重要的六点，也给自己未来的设计评估工作提个醒：

- 开发成本：API 设计是否简洁、文档是否清晰，开发者上手难度（学习曲线）。
- 运维成本：部署、配置、监控是否便捷（如是否有可视化工具、自动化脚本）。
- 问题排查：是否提供完善的日志、监控指标，故障定位的难易程度。
- 社区支持：开源社区活跃度（贡献者数量、issue 响应速度、版本更新频率）。
- 商业支持：是否有厂商提供商业服务（如技术支持、培训），避免依赖单一社区。
- 生命周期：是否处于活跃维护阶段（无停产风险），未来演进方向是否清晰。


